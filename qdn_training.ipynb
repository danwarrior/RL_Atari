{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "qdn_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dguerrero77/RL_Atari/blob/main/qdn_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLkeG5uS1n-X",
        "outputId": "f00c0180-f777-437e-a947-01c2e89e8b6a"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 10.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (51.3.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfxGGguzqEqz",
        "outputId": "a091b564-b125-4b2f-f4f9-15774bb22bc7"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/RL_Atari\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4-UWw9vsmlx",
        "outputId": "f1b5d936-cfff-43d4-d55a-c39b4072d702"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True )\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBl2pvbjtZ2v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37692fa0-6ce3-466b-8128-49af947ad6d8"
      },
      "source": [
        "import os\n",
        "os.chdir('/gdrive/MyDrive/RL_Atari')\n",
        "os.getcwd()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/MyDrive/RL_Atari'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNTyujG7tiDr"
      },
      "source": [
        "from lib import wrappers\n",
        "from lib import dqn_model\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JxXndaRvfCI",
        "outputId": "5de3d3c8-9cc6-4927-e496-be6902999b8d"
      },
      "source": [
        "!ls /gdrive/MyDrive/"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'algoritmos genéticos.gslides'\n",
            "'Clasificación músical por género.gslides'\n",
            "'Colab Notebooks'\n",
            "'Coloquio roadmap.gslides'\n",
            "'Complejidad en lenguajes musicales.gslides'\n",
            " cronograma_S2.gsheet\n",
            "'Data Scientist.gslides'\n",
            "'Deep Neuroevolution: genetic algorithms are a competitive alternative for training DNNs Petroski Such et al. (2018) UBER AI Labs.gslides'\n",
            "'Getting started.pdf'\n",
            "'Instrumentational Complexity.gslides'\n",
            " jackal\n",
            "'Meet Recordings'\n",
            "'Música y complejidad.gslides'\n",
            "'Presentacion Workshop - Bar chart 1.gsheet'\n",
            " RL_Atari\n",
            " Sports\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document.gdoc'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKh0ybE3txS9",
        "outputId": "1a56f4de-0c04-48d3-94e6-8737892488c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True )"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "89YUWJ5W00j-",
        "outputId": "6d1830e2-8a29-4cb5-e744-a47fac16892b"
      },
      "source": [
        "import os\n",
        "os.chdir('/gdrive/MyDrive/RL_Atari')\n",
        "os.getcwd()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/MyDrive/RL_Atari'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYoX8Ig5qErA"
      },
      "source": [
        "from lib import wrappers\n",
        "from lib import dqn_model\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89zeYg2KqErA"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O71M_wzOqErB"
      },
      "source": [
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19.5\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhOJVH6gqErB"
      },
      "source": [
        "Experience = collections.namedtuple('Experience',\n",
        "                                    field_names=['state', 'action', 'reward', 'done', 'new_state'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL849pSxqErB"
      },
      "source": [
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNBhllhHqErC"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n54A9WnMqErD"
      },
      "source": [
        "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pT0WnxDqErD"
      },
      "source": [
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
        "    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                        help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "    parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
        "                        help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
        "    args = parser.parse_args()\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    env = wrappers.make_env(args.env)\n",
        "\n",
        "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "    print(net)\n",
        "\n",
        "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "    epsilon = EPSILON_START\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    total_rewards = []\n",
        "    frame_idx = 0\n",
        "    ts_frame = 0\n",
        "    ts = time.time()\n",
        "    best_mean_reward = None\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon,\n",
        "                speed\n",
        "            ))\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), args.env + \"-best.dat\")\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "                best_mean_reward = mean_reward\n",
        "            if mean_reward > args.reward:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            continue\n",
        "\n",
        "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}